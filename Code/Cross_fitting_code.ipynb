{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51665c4",
   "metadata": {},
   "source": [
    "Code to replicate the cross-fitting simulation in the Master's Thesis Neural Nets for Treatment Effect Estimation by Hugo Foerster-Baldenius. Requires some changes to the Causal Nets package.\n",
    "# Cross-fitting Simulation\n",
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7adf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Stopping Tensorflow from printing info messages\n",
    "# and warnings.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import logging\n",
    "import warnings\n",
    "from causal_nets.input_checker import InputChecker\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from causal_nets import causal_net_estimate\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from savReaderWriter import SavReaderNp\n",
    "import random\n",
    "from more_itertools import sliced\n",
    "from scipy import stats\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "# Stopping deprecation warnings\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "##Implement changes to Causal Nets Library (https://github.com/PopovicMilica/causal_nets) to allow for cross-fitting\n",
    "\n",
    "class _MyLogger(Callback):\n",
    "    '''\n",
    "    Printing validation loss after the first epoch and\n",
    "    then after every n epochs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_epochs: int, optional\n",
    "        After how many epochs to print a validation loss.\n",
    "        Default value is after each 25 epochs.\n",
    "    '''\n",
    "    def __init__(self, n_epochs=25):\n",
    "        super().__init__()\n",
    "        self.after_n_epochs = n_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.after_n_epochs == 0:\n",
    "            print('%d epoch - loss: %.4f  val loss: %.4f' % (\n",
    "                epoch+1, logs['loss'], logs['val_loss']))\n",
    "\n",
    "\n",
    "def set_tensorflow_seed(seed_num):\n",
    "    '''\n",
    "    Set tensorflow seed if provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed: int or None\n",
    "         Tensorflow seed number\n",
    "    '''\n",
    "    if seed_num is not None:\n",
    "        tf.compat.v1.set_random_seed(seed_num)\n",
    "\n",
    "\n",
    "class CoeffNet():\n",
    "    '''\n",
    "    Neural network for causal effect estimation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_layer_sizes: list of ints\n",
    "        Length of the list defines the number of hidden layers.\n",
    "        Entries of the list define the number of hidden units in each\n",
    "        hidden layer. (e.g. hidden_layer_sizes = [60, 30])\n",
    "    dropout_rates: list of floats\n",
    "        Dropout rate for each layer. Each entry has to be between\n",
    "        0 and 1. Has to be of length len(hidden_layer_sizes).\n",
    "    batch_size: int\n",
    "        Batch size.\n",
    "    alpha: float\n",
    "        Regularization strength parameter.\n",
    "    r_par: float\n",
    "        Mixing ratio of Ridge and Lasso regression.\n",
    "        Has to be between 0 and 1. If r_par = 1, than this is equal to\n",
    "        having Lasso regression. If r_par = 0, than it is equal to\n",
    "        having Ridge regression.\n",
    "    optimizer: {'Adam', 'GradientDescent', 'RMSprop'}\n",
    "        Which optimizer to use.\n",
    "    learning_rate: scalar\n",
    "        Learning rate.\n",
    "    max_epochs_without_change: int\n",
    "        Number of epochs with no improvement on the validation loss to\n",
    "        wait before stopping the training.\n",
    "    max_nepochs: int\n",
    "        Maximum number of epochs for which the neural network will\n",
    "        be trained.\n",
    "    seed: int or None\n",
    "        Tensorflow seed number.\n",
    "    nparameters: int\n",
    "        Number of units in the output layer.\n",
    "    verbose: bool\n",
    "        Prints out the model summary and training progress.\n",
    "    '''\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rates,\n",
    "                 batch_size, alpha, r_par, optimizer, learning_rate,\n",
    "                 max_epochs_without_change, max_nepochs, seed, verbose):\n",
    "\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.r_par = r_par\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs_without_change = max_epochs_without_change\n",
    "        self.max_nepochs = max_nepochs\n",
    "        self.verbose = verbose\n",
    "        self.nparameters = 2\n",
    "\n",
    "        # Set tensorflow seed\n",
    "        set_tensorflow_seed(seed)\n",
    "\n",
    "    def _last_layer(self, combined_input):\n",
    "        '''\n",
    "        Building a custom layer which will be appended at the end of\n",
    "        the feed-forward neural network.\n",
    "\n",
    "        This layer will return the value of: tau * T + mu0,\n",
    "        where `tau` is conditional average treatment effect for each\n",
    "        individual, `T` is the treatment for each individual and\n",
    "        `mu0` is estimated target value given x in case of no treatment\n",
    "        for each individual.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        combined_input: Tensor\n",
    "            Concatenated layer of `tau`, `mu0` and `input_t`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        V_values:\n",
    "        '''\n",
    "        import tensorflow as tf\n",
    "        tau = combined_input[:, 0:1]\n",
    "        mu0 = combined_input[:, 1:2]\n",
    "\n",
    "        t = combined_input[:, self.nparameters:]\n",
    "\n",
    "        V_values = tf.multiply(t, tau) + mu0\n",
    "        return V_values\n",
    "\n",
    "    def _last_layer_output_shape(self, input_shape):\n",
    "        '''\n",
    "        Returns the shape of the custom last layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape: Shape of the previous layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The shape of the custom last layer as a tuple.\n",
    "        '''\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) == 2\n",
    "        shape[-1] = 1\n",
    "        return tuple(shape)\n",
    "\n",
    "    def _building_the_model(self, nfeatures):\n",
    "        '''\n",
    "        Build the whole fully connected neural network that estimates\n",
    "        causal coefficients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nfeatures: int\n",
    "            Number of features in the input layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model: keras model\n",
    "            Full keras model that returns estimated target values.\n",
    "        betas_model: keras model\n",
    "            Model, that encapsulates only the feed-forward neural\n",
    "            network, which outputs causal coefficients.\n",
    "        '''\n",
    "        # Matrix of consumer characteristics\n",
    "        input_x = Input(shape=(nfeatures,))\n",
    "\n",
    "        # Array of treatments\n",
    "        input_t = Input(shape=(1,))\n",
    "\n",
    "        reg = keras.regularizers.l1_l2(\n",
    "            l1=self.alpha*self.r_par, l2=self.alpha*(1-self.r_par))\n",
    "\n",
    "        for i in range(len(self.hidden_layer_sizes)):\n",
    "\n",
    "            if i == 0:\n",
    "                output = input_x\n",
    "\n",
    "            output = Dense(self.hidden_layer_sizes[i], activation='relu',\n",
    "                           use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                           kernel_regularizer=reg,\n",
    "                           bias_initializer='zeros')(output)\n",
    "\n",
    "            output = Dropout(self.dropout_rates[i])(output)\n",
    "\n",
    "        betas = Dense(self.nparameters, activation=None, use_bias=True,\n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      kernel_regularizer=reg, bias_initializer='zeros')(output)\n",
    "\n",
    "        combined = concatenate([betas, input_t], axis=-1)\n",
    "\n",
    "        output_tensor = Lambda(\n",
    "            self._last_layer,\n",
    "            output_shape=self._last_layer_output_shape)(combined)\n",
    "\n",
    "        model = Model(inputs=[input_x, input_t], outputs=output_tensor)\n",
    "        betas_model = Model(inputs=input_x, outputs=betas)\n",
    "\n",
    "        if self.optimizer == 'Adam':\n",
    "            opt = keras.optimizers.Adam(lr=self.learning_rate, beta_1=0.9,\n",
    "                                        beta_2=0.999, epsilon=None, decay=0.0,\n",
    "                                        amsgrad=True)\n",
    "        elif self.optimizer == 'GradientDescent':\n",
    "            opt = keras.optimizers.SGD(lr=self.learning_rate, momentum=0.0,\n",
    "                                       decay=0.0, nesterov=False)\n",
    "        else:\n",
    "            opt = keras.optimizers.RMSprop(lr=self.learning_rate, rho=0.9,\n",
    "                                           epsilon=None, decay=0.0)\n",
    "\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "        return model, betas_model\n",
    "\n",
    "    def training_NN(self, training_data, validation_data):\n",
    "        '''\n",
    "        Train a NN for max_nepochs or until early stopping criterion\n",
    "        is met.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data: list of arrays\n",
    "            Data on which the training of the Neural Network will be\n",
    "            performed. It is comprised as a list of arrays, in the\n",
    "            following manner:\n",
    "            [X_train, T_train, Y_train], where `X_train` is an array of\n",
    "            input features, `T_train` is the treatment array, and\n",
    "            `Y_train` is the target array.\n",
    "\n",
    "        validation_data: list of arrays\n",
    "            Data on which the validation of the Neural Network will be\n",
    "            performed. It is composed in the same manner as the\n",
    "            training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        betas_model: keras model\n",
    "            Model, that encapsulates only the feed-forward neural\n",
    "            network, which as an output has causal coefficients.\n",
    "        history_dict: dict\n",
    "            Dictionary that stores validation and training loss values for\n",
    "            CoeffNet.\n",
    "        '''\n",
    "        # Clearing the weights\n",
    "        K.clear_session()\n",
    "\n",
    "        nfeatures = np.shape(training_data[0])[1]\n",
    "        # Building the modeL\n",
    "        model, betas_model = self._building_the_model(nfeatures)\n",
    "\n",
    "        if self.verbose:\n",
    "            model.summary()\n",
    "\n",
    "        EarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=self.max_epochs_without_change,\n",
    "            restore_best_weights=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            # Training the model\n",
    "            print('\\nTraining of the causal coefficients neural network:')\n",
    "            history = model.fit(\n",
    "                x=training_data[0:2], y=training_data[2],\n",
    "                epochs=self.max_nepochs, batch_size=self.batch_size,\n",
    "                validation_data=(validation_data[0:2], validation_data[2]),\n",
    "                callbacks=[EarlyStop, _MyLogger()], shuffle=True, verbose=0)\n",
    "            print('Training is finished.\\n')\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                x=training_data[0:2], y=training_data[2],\n",
    "                epochs=self.max_nepochs, batch_size=self.batch_size,\n",
    "                validation_data=(validation_data[0:2], validation_data[2]),\n",
    "                callbacks=[EarlyStop], shuffle=True, verbose=0)\n",
    "        history_dict = history.history\n",
    "        return betas_model, history_dict\n",
    "\n",
    "    def retrieve_coeffs(self, betas_model, input_value):\n",
    "        '''\n",
    "        After training is completed retrieve coefficient values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        betas_model: keras model\n",
    "            Model, that encapsulates only the feed-forward neural\n",
    "            network, which as an output has causal coefficients.\n",
    "        input_value: array like\n",
    "            Features array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tau_pred: ndarray\n",
    "            Estimated conditional average treatment effect.\n",
    "        mu0_pred: ndarray\n",
    "            Estimated target value given x in case of no treatment.\n",
    "        '''\n",
    "        betas_pred = betas_model.predict(input_value)\n",
    "        tau_pred = betas_pred[:, :1]\n",
    "        mu0_pred = betas_pred[:, 1:2]\n",
    "        return tau_pred, mu0_pred\n",
    "\n",
    "\n",
    "class PropensityScoreNet():\n",
    "    '''\n",
    "    Neural network for propensity scores estimation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Same as in class CoeffNet.\n",
    "    '''\n",
    "    def __init__(self, hidden_layer_sizes, dropout_rates, batch_size,\n",
    "                 alpha, r_par, optimizer, learning_rate,\n",
    "                 max_epochs_without_change, max_nepochs, seed, verbose):\n",
    "\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.r_par = r_par\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs_without_change = max_epochs_without_change\n",
    "        self.max_nepochs = max_nepochs\n",
    "        self.verbose = verbose\n",
    "        self.nparameters = 1\n",
    "\n",
    "        # Set tensorflow seed\n",
    "        set_tensorflow_seed(seed)\n",
    "\n",
    "    def _building_the_model(self, nfeatures):\n",
    "        '''\n",
    "        Build the whole fully connected neural network that estimates\n",
    "        propensity scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nfeatures: int\n",
    "            Number of features in the input layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model: keras model\n",
    "            Keras model which returns estimated propensity scores.\n",
    "        '''\n",
    "        # Matrix of consumer characteristics\n",
    "        input_x = Input(shape=(nfeatures,))\n",
    "\n",
    "        reg = keras.regularizers.l1_l2(\n",
    "            l1=self.alpha*self.r_par, l2=self.alpha*(1-self.r_par))\n",
    "\n",
    "        for i in range(len(self.hidden_layer_sizes)):\n",
    "            if i == 0:\n",
    "                output = input_x\n",
    "\n",
    "            output = Dense(self.hidden_layer_sizes[i], activation='relu',\n",
    "                           use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           kernel_regularizer=reg,\n",
    "                           bias_initializer='zeros')(output)\n",
    "            output = Dropout(self.dropout_rates[i])(output)\n",
    "\n",
    "        ps_outputs = Dense(self.nparameters, activation='sigmoid',\n",
    "                           use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                           kernel_regularizer=reg,\n",
    "                           bias_initializer='zeros')(output)\n",
    "\n",
    "        model = Model(inputs=input_x, outputs=ps_outputs)\n",
    "\n",
    "        if self.optimizer == 'Adam':\n",
    "            opt = keras.optimizers.Adam(lr=self.learning_rate, beta_1=0.9,\n",
    "                                        beta_2=0.999, epsilon=None, decay=0.0,\n",
    "                                        amsgrad=True)\n",
    "        elif self.optimizer == 'GradientDescent':\n",
    "            opt = keras.optimizers.SGD(lr=self.learning_rate, momentum=0.0,\n",
    "                                       decay=0.0, nesterov=False)\n",
    "        else:\n",
    "            opt = keras.optimizers.RMSprop(lr=self.learning_rate, rho=0.9,\n",
    "                                           epsilon=None, decay=0.0)\n",
    "\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "        return model\n",
    "\n",
    "    def training_NN(self, training_data, validation_data):\n",
    "        '''\n",
    "        Train a NN for max_nepochs or until early stopping criterion\n",
    "        is met.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data: list of arrays\n",
    "            Data on which the training of the Neural Network will be\n",
    "            performed. It is comprised as a list of arrays, in the\n",
    "            following manner:\n",
    "            [X_train, T_train], where `X_train` is an array of\n",
    "            input features, `T_train` is the treatment array.\n",
    "\n",
    "        validation_data: list of arrays\n",
    "            Data on which the validation of the Neural Network will be\n",
    "            performed. It is composed in the same manner as the\n",
    "            training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model: keras model\n",
    "            Keras model which returns estimated propensity scores.\n",
    "        history_ps_dict: dict\n",
    "            Dictionary that stores validation and training loss values\n",
    "            for PropensityScoreNet.\n",
    "        '''\n",
    "        # Clearing the weights\n",
    "        K.clear_session()\n",
    "\n",
    "        nfeatures = np.shape(training_data[0])[1]\n",
    "        # Building the modeL\n",
    "        model = self._building_the_model(nfeatures)\n",
    "\n",
    "        if self.verbose:\n",
    "            model.summary()\n",
    "\n",
    "        EarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=self.max_epochs_without_change,\n",
    "            restore_best_weights=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            # Training the model\n",
    "            print('\\nTraining of the propensity score neural network:')\n",
    "            history_ps = model.fit(\n",
    "                x=training_data[0], y=training_data[1],\n",
    "                epochs=self.max_nepochs, batch_size=self.batch_size,\n",
    "                validation_data=(validation_data[0], validation_data[1]),\n",
    "                callbacks=[EarlyStop, _MyLogger()], shuffle=True, verbose=0)\n",
    "            print('Training is finished.')\n",
    "        else:\n",
    "            history_ps = model.fit(\n",
    "                x=training_data[0], y=training_data[1],\n",
    "                epochs=self.max_nepochs, batch_size=self.batch_size,\n",
    "                validation_data=(validation_data[0], validation_data[1]),\n",
    "                callbacks=[EarlyStop], shuffle=True, verbose=0)\n",
    "\n",
    "        history_ps_dict = history_ps.history\n",
    "        return model, history_ps_dict\n",
    "\n",
    "    def retrieve_propensity_scores(self, model, input_value):\n",
    "        '''\n",
    "        After training is completed retrieve propensity scores for\n",
    "        a given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: keras model\n",
    "            Keras model which returns estimated propensity scores.\n",
    "        input_value: array like\n",
    "            Features array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prob_of_t_pred: ndarray\n",
    "            Estimated propensity scores\n",
    "        '''\n",
    "        prob_of_t_pred = model.predict(input_value)\n",
    "        return prob_of_t_pred\n",
    "\n",
    "\n",
    "def determine_batch_size(batch_size, training_data):\n",
    "    '''\n",
    "    Assign batch size value if the batch size is not provided.\n",
    "    If batch_size is None, than batch size is equal to the length of\n",
    "    the training dataset for training datasets with less than 50000 rows.\n",
    "    Otherwise, it is set to 1024.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size: int or None\n",
    "        Batch size.\n",
    "    training_data: list of arrays\n",
    "        Data on which the training of the neural network should be\n",
    "        performed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_size: int\n",
    "        Batch size.\n",
    "    '''\n",
    "    if len(training_data[0]) > 50000:\n",
    "        batch_size = 1024\n",
    "    else:\n",
    "        batch_size = len(training_data[0])\n",
    "    return batch_size\n",
    "\n",
    "\n",
    "def determine_dropout_rates(hidden_layer_sizes):\n",
    "    '''\n",
    "    Sets all dropout rates to zero if dropout rates are not provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_layer_sizes: list of ints\n",
    "        Length of the list defines the number of hidden layers.\n",
    "        Entries of the list define the number of hidden units in each\n",
    "        hidden layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dropout rates list of appropriate length with all values\n",
    "    set to zero.\n",
    "    '''\n",
    "    return [0]*len(hidden_layer_sizes)\n",
    "\n",
    "\n",
    "def _influence_functions(mu0_pred, tau_pred, Y, T,\n",
    "                         prob_t_pred, estimate_ps):\n",
    "    '''\n",
    "    Calculate the target value for each individual when treatment is\n",
    "    0 or 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu0_pred: ndarray\n",
    "        Estimated target value given x in case of no treatment.\n",
    "    tau_pred: ndarray\n",
    "        Estimated conditional average treatment effect.\n",
    "    Y: ndarray\n",
    "        Target value array.\n",
    "    T: ndarray\n",
    "        Treatment array.\n",
    "    prob_t_pred: ndarray\n",
    "        Estimated propensity scores.\n",
    "    estimate_ps: bool\n",
    "        Should the propensity scores be estimated or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    psi_0: ndarray\n",
    "        Influence function for given x in case of no treatment.\n",
    "    psi_1: ndarray\n",
    "        Influence function for given x in case of treatment.\n",
    "    '''\n",
    "    T = np.array(T).reshape(-1, 1)\n",
    "    Y = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "    first_part = (1-T) * (Y-mu0_pred)\n",
    "    second_part = T * (Y-mu0_pred-tau_pred)\n",
    "\n",
    "    if estimate_ps:\n",
    "        prob_t_pred[prob_t_pred < 0.0001] = 0.0001\n",
    "        prob_t_pred[prob_t_pred > 0.9999] = 0.9999\n",
    "        psi_0 = (first_part/(1-prob_t_pred)) + mu0_pred\n",
    "        psi_1 = (second_part/prob_t_pred) + mu0_pred + tau_pred\n",
    "    else:\n",
    "        psi_0 = (first_part/(1-np.mean(T))) + mu0_pred\n",
    "        psi_1 = (second_part/np.mean(T)) + mu0_pred + tau_pred\n",
    "    return psi_0, psi_1\n",
    "\n",
    "\n",
    "def causal_net_estimate(training_data, validation_data, test_data, prediction_data,\n",
    "                        hidden_layer_sizes, dropout_rates=None,\n",
    "                        batch_size=None, alpha=0., r_par=0., optimizer='Adam',\n",
    "                        learning_rate=0.0009, max_epochs_without_change=30,\n",
    "                        max_nepochs=5000, seed=None, estimate_ps=False,\n",
    "                        verbose=True, hidden_layer_sizes_t=None,\n",
    "                        dropout_rates_t=None, batch_size_t=None, alpha_t=0.,\n",
    "                        r_par_t=0., optimizer_t='Adam', learning_rate_t=0.0009,\n",
    "                        max_epochs_without_change_t=30, max_nepochs_t=5000,\n",
    "                        seed_t=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_data: list of arrays\n",
    "        Data on which the training of the Neural Network will be\n",
    "        performed. It must be comprised as a list of arrays, in the\n",
    "        following manner: [X_train, T_train, Y_train]\n",
    "        Here, `X_train` is an array of input features, `T_train` is\n",
    "        the treatment array, and `Y_train` is the target array.\n",
    "    validation_data: list of arrays\n",
    "        Data on which the validation of the Neural Network will be\n",
    "        performed. It has to be composed in the same manner as the\n",
    "        training data.\n",
    "    test_data: list of arrays\n",
    "        Data on which we want to perform estimation. It has to be\n",
    "        composed in the same manner as the training and validation data.\n",
    "    hidden_layer_sizes: list of ints\n",
    "        `hidden_layer_sizes` is a list that defines a size and width of\n",
    "        the neural network that estimates causal coefficients. Length of\n",
    "        the list defines the number of hidden layers. Entries of the\n",
    "        list define the number of hidden units in each hidden layer.\n",
    "        No default value, it needs to be provided.\n",
    "        E.g. hidden_layer_sizes = [60, 30]\n",
    "    dropout_rates: list of floats or None, optional\n",
    "        If it's a list than the values of the list represent dropout\n",
    "        rate for each layer of the feed-forward neural network\n",
    "        that estimates causal coefficients. Each entry of the list has\n",
    "        to be between 0 and 1. Also, list has to be of the same length\n",
    "        as the list 'hidden_layer_sizes'. If is set to None, than\n",
    "        dropout is not applied. Default value is None.\n",
    "    batch_size: int, optional\n",
    "        Batch size for the neural network that estimates causal\n",
    "        coefficients. Default value is None. If batch_size is None,\n",
    "        than batch size is equal to length of the training dataset for\n",
    "        training datasets smaller than 50000 rows and set to 1024 for\n",
    "        larger datasets. Otherwise, it is equal to the value provided.\n",
    "    alpha: float, optional\n",
    "        Regularization strength parameter for the neural network that\n",
    "        estimates causal coefficients. Default value is 0.\n",
    "    r_par: float, optional\n",
    "        Mixing ratio of Ridge and Lasso regression for the neural\n",
    "        network that estimates causal coefficients.\n",
    "        Has to be between 0 and 1. If r_par = 1, than this is equal to\n",
    "        having Lasso regression. If r_par = 0, than it is equal to\n",
    "        having Ridge regression. Default value is 0.\n",
    "    optimizer: {'Adam', 'GradientDescent', 'RMSprop'}, optional\n",
    "        Which optimizer to use for the neural network that estimates\n",
    "        causal coefficients. Default: 'Adam'.\n",
    "    learning_rate: scalar, optional\n",
    "        Learning rate for the neural network that estimates\n",
    "        causal coefficients. Default value is 0.0009.\n",
    "    max_epochs_without_change: int, optional\n",
    "        Number of epochs with no improvement on the validation loss to\n",
    "        wait before stopping the training for the neural network that\n",
    "        estimates causal coefficients. Default value is 30.\n",
    "    max_nepochs: int, optional\n",
    "        Maximum number of epochs for which neural network that\n",
    "        estimates causal coefficients will be trained.\n",
    "        Default value is 5000.\n",
    "    seed: int or None, optional\n",
    "        Tensorflow seed number for the neural network that estimates\n",
    "        causal coefficients. Default value is None.\n",
    "    estimate_ps: bool, optional\n",
    "        Should the propensity scores be estimated or not. If the\n",
    "        treatment is randomized then this variable should be set to\n",
    "        False. In not randomized treatment case, it should be set to\n",
    "        True. Default value is False.\n",
    "    verbose: bool, optional\n",
    "        Should the model summary and losses during training be printed.\n",
    "        If it is set to False, the printing behavior is suppressed.\n",
    "        Default value is True.\n",
    "    hidden_layer_sizes_t: list of ints or None, optional\n",
    "        `hidden_layer_sizes_t` is a list that defines a size and width\n",
    "        of the neural network that estimates propensity scores. Length\n",
    "        of the list defines the number of hidden layers. Entries of the\n",
    "        list define the number of hidden units in each hidden layer.\n",
    "        Default value is None, but if 'estimate_ps' is set to True,\n",
    "        than the values for this argument needs to be provided.\n",
    "        E.g. hidden_layer_sizes_t = [60, 30]\n",
    "    dropout_rates_t: list of floats or None, optional\n",
    "        If it's a list than the values of the list represent dropout\n",
    "        rate for each layer of the neural network that estimates\n",
    "        propensity scores. Each entry of the list has to be between 0\n",
    "        and 1. Also, list has to be of same length as the list\n",
    "        'hidden_layer_sizes_t'. If is set to None, than dropout is not\n",
    "        applied.\n",
    "        Default value is None.\n",
    "    batch_size_t: int, optional\n",
    "        Batch size for the neural network that estimates propensity\n",
    "        scores. Default value is None. If batch_size is None, than\n",
    "        batch size is equal to the length of the training dataset for\n",
    "        training datasets smaller than 50000 rows and set to 1024 for\n",
    "        larger datasets. Otherwise, it is equal to the value provided.\n",
    "    alpha_t: float, optional\n",
    "        Regularization strength parameter for the neural network that\n",
    "        estimates propensity scores. Default value is 0.\n",
    "    r_par_t: float, optional\n",
    "        Mixing ratio of Ridge and Lasso regression for the neural\n",
    "        network that estimates propensity scores.\n",
    "        Has to be between 0 and 1. If r_par_t = 1, than this is equal to\n",
    "        having Lasso regression. If r_par_t = 0, than it is equal to\n",
    "        having Ridge regression. Default value is 0.\n",
    "    optimizer_t: {'Adam', 'GradientDescent', 'RMSprop'}, optional\n",
    "        Which optimizer to use for the neural network that estimates\n",
    "        propensity scores. Default: 'Adam'.\n",
    "    learning_rate_t: scalar, optional\n",
    "        Learning rate for the neural network that estimates propensity\n",
    "        scores. Default value is 0.0009.\n",
    "    max_epochs_without_change_t: int, optional\n",
    "        Number of epochs with no improvement on the validation loss to\n",
    "        wait before stopping the training for the neural network that\n",
    "        estimates propensity scores. Default value is 30.\n",
    "    max_nepochs_t: int, optional\n",
    "        Maximum number of epochs for which neural network, that\n",
    "        estimates propensity scores, will be trained.\n",
    "        Default value is 5000.\n",
    "    seed_t: int or None, optional\n",
    "        Tensorflow seed number for the neural network that estimates\n",
    "        propensity scores. Default value is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tau_pred: ndarray\n",
    "        Estimated conditional average treatment effect.\n",
    "    mu0_pred: ndarray\n",
    "        Estimated target value given x in case of no treatment.\n",
    "    prob_t_pred: ndarray\n",
    "        Estimated propensity scores.\n",
    "    psi_0: ndarray\n",
    "        Influence function for given x in case of no treatment.\n",
    "    psi_1: ndarray\n",
    "        Influence function for given x in case of treatment.\n",
    "    history_dict: dict\n",
    "        Dictionary that stores validation and training loss values for\n",
    "        CoeffNet.\n",
    "    history_ps_dict: dict\n",
    "        Dictionary that stores validation and training loss values for\n",
    "        PropensityScoreNet. If estimate_ps is set to None,\n",
    "        history_ps_dict is set to None as well.\n",
    "    '''\n",
    "    # Check that all the inputs to causal_net_estimate function are valid\n",
    "#    input_checker = InputChecker(\n",
    "#        training_data, validation_data, test_data, hidden_layer_sizes,\n",
    "#        dropout_rates, batch_size, alpha, r_par, optimizer, learning_rate,\n",
    "#        max_epochs_without_change, max_nepochs, seed, estimate_ps, verbose,\n",
    "#        hidden_layer_sizes_t, dropout_rates_t, batch_size_t, alpha_t, r_par_t,\n",
    "#        optimizer_t, learning_rate_t, max_epochs_without_change_t,\n",
    "#        max_nepochs_t, seed_t)\n",
    "\n",
    "#    input_checker.check_all_parameters()\n",
    "\n",
    "    if batch_size is None:\n",
    "        batch_size = determine_batch_size(batch_size, training_data)\n",
    "    if dropout_rates is None:\n",
    "        dropout_rates = determine_dropout_rates(hidden_layer_sizes)\n",
    "\n",
    "    coeff_net = CoeffNet(hidden_layer_sizes, dropout_rates, batch_size,\n",
    "                         alpha, r_par, optimizer, learning_rate,\n",
    "                         max_epochs_without_change, max_nepochs, seed, verbose)\n",
    "\n",
    "    model_coeff_net, history_dict = coeff_net.training_NN(\n",
    "        training_data, validation_data)\n",
    "    tau_pred, mu0_pred = coeff_net.retrieve_coeffs(\n",
    "        model_coeff_net, test_data[0])\n",
    "    tau_pred_cross, mu0_pred_cross = coeff_net.retrieve_coeffs(model_coeff_net, prediction_data)\n",
    "\n",
    "    if estimate_ps:\n",
    "        if batch_size_t is None:\n",
    "            batch_size_t = determine_batch_size(batch_size_t, training_data)\n",
    "        if dropout_rates_t is None:\n",
    "            dropout_rates_t = determine_dropout_rates(hidden_layer_sizes_t)\n",
    "\n",
    "        ps_net = PropensityScoreNet(\n",
    "            hidden_layer_sizes_t, dropout_rates_t, batch_size_t, alpha_t,\n",
    "            r_par_t, optimizer_t, learning_rate_t,\n",
    "            max_epochs_without_change_t, max_nepochs_t, seed_t, verbose)\n",
    "\n",
    "        model_ps_net, history_ps_dict = ps_net.training_NN(\n",
    "            training_data[0:2], validation_data[0:2])\n",
    "        prob_t_pred = ps_net.retrieve_propensity_scores(\n",
    "            model_ps_net, test_data[0])\n",
    "    else:\n",
    "        prob_t_pred = np.mean(test_data[1])\n",
    "        history_ps_dict = None\n",
    "\n",
    "    psi_0, psi_1 = _influence_functions(mu0_pred, tau_pred,\n",
    "                                        test_data[2],\n",
    "                                        test_data[1],\n",
    "                                        prob_t_pred, estimate_ps)\n",
    "    return tau_pred, mu0_pred, prob_t_pred, psi_0, psi_1, history_dict,\\\n",
    "        history_ps_dict, tau_pred_cross, mu0_pred_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bccfa",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d66f649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###so general plan is to split the sample into k folds ... estimate the nuisance functions on the folds\n",
    "###and estimate the influence function on the counterpart to the fold\n",
    "###and then average\n",
    "\n",
    "##functions for simulation 1\n",
    "def loop_inside_cross(number, ATT, N, runs, sample, model, nconsumer_characteristics, hidden_layer_sizes, estimate_ps, alpha_p, alpha_mu, alpha_tau,dropout_rates):\n",
    "#def loop_inside_cross( ATT, N, runs, sample, model, nconsumer_characteristics, hidden_layer_sizes, estimate_ps, alpha_p, alpha_mu, alpha_tau,dropout_rates):\n",
    "\n",
    "        seed = random.randint(1, 100000)\n",
    "        np.random.seed(seed)\n",
    "        X = np.hstack((np.repeat(1,N).reshape(N,1), np.random.uniform(low=0, high=1, size=[N, nconsumer_characteristics])))\n",
    "        if sample=='randomized':\n",
    "            prob_of_T = 0.5\n",
    "        else:\n",
    "            p_of_t = np.dot(X[:, :21], alpha_p)\n",
    "            p_of_t = np.reshape(p_of_t, -1)\n",
    "            p_of_t = p_of_t.reshape(-1)\n",
    "            prob_of_T = 1/(1+np.exp(-p_of_t))\n",
    "\n",
    "\n",
    "        T = np.random.binomial(size=N, n=1, p=prob_of_T)\n",
    "\n",
    "        epsilon= np.random.normal(0, 1, N)\n",
    "\n",
    "        mu0_real= np.dot(X, alpha_mu.T) \n",
    "        tau_real = np.dot(X, alpha_tau) \n",
    "\n",
    "        if model == 'quadratic':\n",
    "                count = comb(nconsumer_characteristics, 2, True, True)\n",
    "                beta_tau = np.random.uniform(low=-0.05, high=0.06, size=count)\n",
    "                tau_real = tau_real + _sum_polynomial_X_times_weights(X, beta_tau)\n",
    "                beta_mu0 = np.random.normal(loc=0.01, scale=0.3, size=count)\n",
    "                mu0_real = mu0_real +_sum_polynomial_X_times_weights(X, beta_mu0)\n",
    "        else:\n",
    "                beta_tau = None\n",
    "                beta_mu = None\n",
    "\n",
    "        if estimate_ps == False:\n",
    "            hidden_layer_sizes_t= None \n",
    "        else:\n",
    "            hidden_layer_sizes_t=[30]\n",
    "            \n",
    "\n",
    "        Y = mu0_real + tau_real*T + epsilon\n",
    "        \n",
    "        ##Split the sample into 5 folds, perform training on the auxiliary of the fold, estimate influence functions on the fold\n",
    "        df=pd.DataFrame({'Y': Y,'T': T})\n",
    "        X=pd.DataFrame(X)\n",
    "        df=pd.concat([df,X],axis=1)\n",
    "        CHUNK_SIZE = 200\n",
    "        index_slices = sliced(list(range(0,len(df))), CHUNK_SIZE)\n",
    "        ate_bin=0\n",
    "        psi_bin=np.empty([N,1])\n",
    "        ticker=0\n",
    "        for index_slice in index_slices:\n",
    "            \n",
    "            chunk = df.drop(index_slice,axis=0) # dataframe chunk ready for use\n",
    "            aux_chunk= df.iloc[index_slice]\n",
    "    \n",
    "            X_frame= chunk.drop(['Y', 'T'], axis=1)\n",
    "            X_train, X_valid, T_train, T_valid, Y_train, Y_valid = train_test_split(X_frame, chunk[\"T\"], chunk.Y, test_size=0.1, random_state=42)\n",
    "            \n",
    "            #prediction_data=aux_chunk.drop([\"Y\",\"T\"],axis=1)\n",
    "            prediction_data=X\n",
    "            \n",
    "            tau_pred, mu0_pred, prob_t_pred, psi_0, psi_1, history, history_ps, tau_pred_cross, mu0_pred_cross = causal_net_estimate(\n",
    "            [X_train, T_train, Y_train], [X_valid, T_valid, Y_valid], [X_frame, chunk[\"T\"], chunk.Y], prediction_data, hidden_layer_sizes, hidden_layer_sizes_t=hidden_layer_sizes_t,\n",
    "            dropout_rates=dropout_rates, batch_size=None, alpha=0., r_par=0., optimizer='Adam', learning_rate=0.0009,\n",
    "            max_epochs_without_change=30, max_nepochs=10000, seed=None, estimate_ps=estimate_ps, verbose=False)\n",
    "\n",
    "            #from the characteristics of the uniform distribution on [0,1] the expected value is .5\n",
    "            if ATT == False:\n",
    "                # Calculate the average treatment effect\n",
    "                mu0_pred_cross=pd.DataFrame(mu0_pred_cross).iloc[index_slice]\n",
    "                tau_pred_cross=pd.DataFrame(tau_pred_cross).iloc[index_slice]\n",
    "                first_part = (1-aux_chunk[\"T\"]) * (aux_chunk.Y-mu0_pred_cross[0])\n",
    "                second_part = aux_chunk[\"T\"] * (aux_chunk.Y-mu0_pred_cross[0]-tau_pred_cross[0])\n",
    "                \n",
    "                if estimate_ps:\n",
    "                    prob_t_pred[prob_t_pred < 0.0001] = 0.0001\n",
    "                    prob_t_pred[prob_t_pred > 0.9999] = 0.9999\n",
    "                    psi_0 = (first_part/(1-prob_t_pred)) + mu0_pred\n",
    "                    psi_1 = (second_part/prob_t_pred) + mu0_pred + tau_pred\n",
    "                else:\n",
    "                    psi_0 = (first_part/(1-np.mean(chunk[\"T\"]))) + mu0_pred_cross[0]\n",
    "                    psi_1 = (second_part/np.mean(chunk[\"T\"])) + mu0_pred_cross[0] + tau_pred_cross[0]\n",
    "                ate_bin += np.mean(psi_1-psi_0)\n",
    "                if ticker==0:\n",
    "                    psi_bin_1=psi_1\n",
    "                    psi_bin_0=psi_0\n",
    "                    ticker=1\n",
    "                    #print(psi_bin)\n",
    "                else:\n",
    "                    psi_bin_1 = np.concatenate((psi_bin_1, psi_1))\n",
    "                    psi_bin_0 = np.concatenate((psi_bin_0, psi_0))\n",
    "                \n",
    "                print(ate_bin)\n",
    "                #print(psi_bin)\n",
    "        ate=ate_bin/5\n",
    "        tau_true_mean = np.sum(0.5 * alpha_tau) + .5*alpha_tau[0]\n",
    "        print(psi_bin_0)\n",
    "        print(psi_bin_1)\n",
    "        mse = np.power((tau_true_mean-ate),2)\n",
    "        # Calculate the bias\n",
    "        # Calculate the 95% confidence interval for average treatment effect\n",
    "        CI_lowerbound = ate - norm.ppf(0.975)*np.std(psi_bin_1-psi_bin_0)/np.sqrt(len(psi_bin_0))\n",
    "        CI_upperbound = ate + norm.ppf(0.975)*np.std(psi_bin_1-psi_bin_0)/np.sqrt(len(psi_bin_0))\n",
    "\n",
    "        IL=CI_upperbound-CI_lowerbound\n",
    "\n",
    "        # How to do coverage??\n",
    "        if tau_true_mean>CI_lowerbound and tau_true_mean<CI_upperbound:\n",
    "            coverage = 1 \n",
    "        else:\n",
    "            coverage = 0\n",
    "\n",
    "        return mse, ate, IL, coverage, tau_true_mean\n",
    "\n",
    "        if ATT == True:\n",
    "\n",
    "            part_one_one= (T*(mu0_pred+ tau_pred))/np.mean(T)\n",
    "            part_one_zero= (T*(mu0_pred))/np.mean(T)\n",
    "            part_two_one= ((prob_t_pred)/np.mean(T))*(T*((Y-mu0_pred-tau_pred)/prob_t_pred))\n",
    "            part_two_zero= ((prob_t_pred)/np.mean(T))*((1-T)*((Y-mu0_pred)/(1-prob_t_pred)))                                                          \n",
    "\n",
    "            psi_one=part_one_one+part_two_one\n",
    "            psi_zero=part_one_zero+part_two_zero\n",
    "            att = np.mean(psi_one-psi_zero)\n",
    "            mse = np.power((tau_true_mean-att),2)\n",
    "\n",
    "            # Calculate the 95% confidence interval for average treatment effect\n",
    "            CI_lowerbound = att - norm.ppf(0.975)*np.std(psi_one-psi_zero)/np.sqrt(len(psi_one))\n",
    "            CI_upperbound = att + norm.ppf(0.975)*np.std(psi_one-psi_zero)/np.sqrt(len(psi_zero))\n",
    "\n",
    "            IL=CI_upperbound-CI_lowerbound\n",
    "\n",
    "            # How to do coverage??\n",
    "            if tau_true_mean>CI_lowerbound and tau_true_mean<CI_upperbound:\n",
    "                coverage = 1 \n",
    "            else:\n",
    "                coverage = 0\n",
    "\n",
    "            return mse, att, IL, coverage, tau_true_mean\n",
    "        \n",
    "def simulator_cross(ATT, N, runs, sample, model, nconsumer_characteristics, hidden_layer_sizes, estimate_ps, dropout_rates):\n",
    "    coverage=[]\n",
    "    mse=[]\n",
    "    IL=[]\n",
    "    ##to always use same DGP\n",
    "    np.random.seed(42)\n",
    "    alpha_p=np.append(0.09, np.random.uniform(-.55,.55,size=[nconsumer_characteristics,1]))\n",
    "    alpha_mu = np.append(.09, np.random.normal(loc=0.3, scale=0.7, size=[1, nconsumer_characteristics]))\n",
    "    alpha_tau=np.append(-0.05,np.random.uniform(0.1,0.22,20))\n",
    "\n",
    "    #mse, ate, IL, coverage, tau_true_mean =loop_inside_cross(ATT, N, runs, sample, model, nconsumer_characteristics, hidden_layer_sizes, estimate_ps, alpha_p, alpha_mu, alpha_tau, dropout_rates)\n",
    "    r = Parallel(n_jobs=4)(delayed(loop_inside_cross)(number, ATT, N, runs, sample, model, nconsumer_characteristics, hidden_layer_sizes, estimate_ps, alpha_p, alpha_mu, alpha_tau, dropout_rates) for number in range(1, runs+1))\n",
    "    mse, ate, IL, coverage, tau_true_mean = zip(*r)     \n",
    "    return mse, ate, IL, coverage, tau_true_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96305336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(se, ate_real, ate, IL, coverage):\n",
    "    mse= np.mean(se)\n",
    "    bias=np.mean(ate)-ate_real[1]\n",
    "    IL=np.mean(IL)\n",
    "    coverage=np.mean(coverage)\n",
    "    return(mse,bias,IL,coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88ec96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from causal_nets import causal_net_estimate\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from savReaderWriter import SavReaderNp\n",
    "import random\n",
    "from more_itertools import sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89944471",
   "metadata": {},
   "outputs": [],
   "source": [
    "se2_1, ate2_1, IL2_1, coverage2_1, tau_true_mean2_1 =simulator_cross(False, N=1000, runs=1000, model='linear',sample='randomized', nconsumer_characteristics=20, hidden_layer_sizes=[20,15,5],estimate_ps=False, dropout_rates=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5f99163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "se2_2, ate2_2, IL2_2, coverage2_2, tau_true_mean2_2 =simulator_cross(False, N=1000, runs=1000, model='linear',sample='randomized', nconsumer_characteristics=20, hidden_layer_sizes=[60,30,20],estimate_ps=False, dropout_rates=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "403fc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "se2_3, ate2_3, IL2_3, coverage2_3, tau_true_mean2_3 =simulator_cross(False, N=1000, runs=1000, model='linear',sample='randomized', nconsumer_characteristics=20, hidden_layer_sizes=[80,80,80],estimate_ps=False, dropout_rates=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14b49734",
   "metadata": {},
   "outputs": [],
   "source": [
    "se2_4, ate2_4, IL2_4, coverage2_4, tau_true_mean2_4 =simulator_cross(False, N=1000, runs=1000, model='linear',sample='randomized', nconsumer_characteristics=20, hidden_layer_sizes=[20,15,10,5],estimate_ps=False, dropout_rates=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e538b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "se2_5, ate2_5, IL2_5, coverage2_5, tau_true_mean2_5 =simulator_cross(False, N=1000, runs=1000, model='linear',sample='randomized', nconsumer_characteristics=20, hidden_layer_sizes=[60,30,20,10],estimate_ps=False, dropout_rates=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0049bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_cv, bias_cv, IL_cv, coverage_cv=calculate_statistics(se2_1, tau_true_mean2_1, ate2_1, IL2_1, coverage2_1)\n",
    "mse2_cv, bias2_cv, IL2_cv, coverage2_cv=calculate_statistics(se2_2, tau_true_mean2_2, ate2_2, IL2_2, coverage2_2)\n",
    "mse3_cv, bias3_cv, IL3_cv, coverage3_cv=calculate_statistics(se2_3, tau_true_mean2_3, ate2_3, IL2_3, coverage2_3)\n",
    "mse4_cv, bias4_cv, IL4_cv, coverage4_cv=calculate_statistics(se2_4, tau_true_mean2_4, ate2_4, IL2_4, coverage2_4)\n",
    "mse5_cv, bias5_cv, IL5_cv, coverage5_cv=calculate_statistics(se2_5, tau_true_mean2_5, ate2_5, IL2_5, coverage2_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3df8af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse4_cv, bias4_cv, IL4_cv, coverage4_cv=calculate_statistics(se2_4, tau_true_mean2_4, ate2_4, IL2_4, coverage2_4)\n",
    "mse5_cv, bias5_cv, IL5_cv, coverage5_cv=calculate_statistics(se2_5, tau_true_mean2_5, ate2_5, IL2_5, coverage2_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "572ba241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00462 -0.00308 0.26582 0.93800\n",
      "0.00440 0.00137 0.25850 0.93400\n",
      "0.00455 -0.00516 0.25824 0.94100\n",
      "0.00471 -0.00041 0.26686 0.95300\n",
      "0.00411 0.00255 0.25882 0.95800\n"
     ]
    }
   ],
   "source": [
    "print(\"%.5f\" % mse_cv, \"%.5f\" % bias_cv, \"%.5f\" %IL_cv, \"%.5f\" %coverage_cv)\n",
    "print(\"%.5f\" % mse2_cv, \"%.5f\" % bias2_cv,\"%.5f\" % IL2_cv, \"%.5f\" %coverage2_cv)\n",
    "print(\"%.5f\" % mse3_cv, \"%.5f\" % bias3_cv, \"%.5f\" %IL3_cv, \"%.5f\" %coverage3_cv)\n",
    "print(\"%.5f\" % mse4_cv, \"%.5f\" % bias4_cv, \"%.5f\" %IL4_cv, \"%.5f\" %coverage4_cv)\n",
    "print(\"%.5f\" % mse5_cv, \"%.5f\" % bias5_cv, \"%.5f\" %IL5_cv, \"%.5f\" %coverage5_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
