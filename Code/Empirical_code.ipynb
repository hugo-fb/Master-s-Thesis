{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc0c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from causal_nets import causal_net_estimate\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from savReaderWriter import SavReaderNp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62f061",
   "metadata": {},
   "source": [
    "Code to implement the application of neural nets to the NSW dataset.\n",
    "# Empirical\n",
    "## NSW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d37ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def NSW_analyzer(Y,T,X,hidden_layer_sizes,hidden_layer_sizes_t):\n",
    "    X_train, X_valid, T_train, T_valid, Y_train, Y_valid = train_test_split(\n",
    "        X, T, Y, test_size=0.2, random_state=42)\n",
    "    dropout_rates_t=[.2,.2,.2]\n",
    "    dropout_rates=None\n",
    "    #dropout_rates_t=None\n",
    "    tau_pred, mu0_pred, prob_t_pred, psi_0, psi_1, history, history_ps = causal_net_estimate(\n",
    "        [X_train, T_train, Y_train], [X_valid, T_valid, Y_valid], [X, T, Y], hidden_layer_sizes=hidden_layer_sizes,\n",
    "        dropout_rates_t=dropout_rates_t, dropout_rates=dropout_rates, \n",
    "        batch_size=None,batch_size_t=None, alpha=0., r_par=0., optimizer='Adam', learning_rate=0.0009,  \n",
    "        hidden_layer_sizes_t=hidden_layer_sizes_t, max_epochs_without_change=30, max_nepochs=10000, seed=42, estimate_ps=True, verbose=True)\n",
    "    mu0_pred = np.concatenate(mu0_pred)\n",
    "    tau_pred=np.concatenate(tau_pred)\n",
    "    prob_t_pred = np.concatenate(prob_t_pred)\n",
    "    prob_t_pred[prob_t_pred < 0.0001] = 0.0001\n",
    "    prob_t_pred[prob_t_pred > 0.9999] = 0.9999\n",
    "    support_frame=pd.DataFrame({'T':T, 'Y':Y, 'mu0_pred':mu0_pred,'tau_pred':tau_pred,'prob_t_pred':prob_t_pred})\n",
    "    half=support_frame[support_frame['T'] == 1]\n",
    "    min_support=min(half[\"prob_t_pred\"])\n",
    "    max_support=max(half[\"prob_t_pred\"])\n",
    "    support_frame = support_frame.drop(support_frame[support_frame[\"prob_t_pred\"]<min_support].index)\n",
    "    support_frame = support_frame.drop(support_frame[support_frame[\"prob_t_pred\"]>max_support].index)\n",
    "    \n",
    "    p=np.mean(support_frame[\"prob_t_pred\"])\n",
    "    first_part = (1-support_frame[\"T\"]) * (support_frame[\"Y\"]-support_frame[\"mu0_pred\"])\n",
    "    second_part = support_frame[\"T\"] * (support_frame[\"Y\"]-support_frame[\"mu0_pred\"]-support_frame[\"tau_pred\"])\n",
    "\n",
    "\n",
    "    ###define ATT from farrell 2015\n",
    "\n",
    "    part_one_one= (support_frame[\"T\"]*(support_frame[\"mu0_pred\"]+support_frame[\"tau_pred\"]))/np.mean(support_frame[\"T\"])\n",
    "\n",
    "    part_one_zero= (support_frame[\"T\"]*(support_frame[\"mu0_pred\"]))/np.mean(support_frame[\"T\"])\n",
    "    \n",
    "    part_two_one= ((support_frame[\"prob_t_pred\"])/np.mean(support_frame[\"T\"]))*(support_frame[\"T\"]*((support_frame[\"Y\"]-support_frame[\"mu0_pred\"]-support_frame[\"tau_pred\"])/support_frame[\"prob_t_pred\"]))\n",
    "    part_two_zero= ((support_frame[\"prob_t_pred\"])/np.mean(support_frame[\"T\"]))*(((1-support_frame[\"T\"])*((support_frame[\"Y\"]-support_frame[\"mu0_pred\"])))/(1-support_frame[\"prob_t_pred\"]))                                                         \n",
    "\n",
    "    psi_one= part_one_one + part_two_one \n",
    "    psi_zero= part_one_zero + part_two_zero \n",
    "                                                                                          \n",
    "    te=np.mean(psi_one-psi_zero)\n",
    "    se=np.std(psi_one - psi_zero)/np.sqrt(len(psi_one))\n",
    "    return(te,se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c8065",
   "metadata": {},
   "source": [
    "### 1. Experimental Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f6b99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSW_1 = pd.read_stata(\"NSW/nsw_dw.dta\")\n",
    "NSW_1.head()\n",
    "Y=NSW_1[\"re78\"]\n",
    "T=NSW_1[\"treat\"]\n",
    "X=NSW_1.iloc[:,2:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "1879213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           180         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           315         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 5)            80          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5)            0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            12          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 587\n",
      "Trainable params: 587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 69548392.0000  val loss: 81807856.0000\n",
      "26 epoch - loss: 68054712.0000  val loss: 78327408.0000\n",
      "51 epoch - loss: 64107484.0000  val loss: 70148088.0000\n",
      "76 epoch - loss: 62549476.0000  val loss: 67569608.0000\n",
      "101 epoch - loss: 62091080.0000  val loss: 67228880.0000\n",
      "126 epoch - loss: 61635168.0000  val loss: 67319512.0000\n",
      "Training is finished.\n",
      "\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training of the propensity score neural network:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - loss: 62.2077  val loss: 12.8453\n",
      "26 epoch - loss: 43.3580  val loss: 3.5010\n",
      "51 epoch - loss: 11.1316  val loss: 2.0803\n",
      "76 epoch - loss: 9.3839  val loss: 1.1886\n",
      "101 epoch - loss: 6.1703  val loss: 1.0012\n",
      "126 epoch - loss: 4.3406  val loss: 0.8980\n",
      "151 epoch - loss: 4.3053  val loss: 0.7474\n",
      "176 epoch - loss: 4.1298  val loss: 0.6797\n",
      "201 epoch - loss: 4.3506  val loss: 0.6875\n",
      "Training is finished.\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "144739.015625\n",
      "-0.0009765625\n",
      "0         72.837563\n",
      "1         48.169762\n",
      "2         64.141327\n",
      "3         59.410980\n",
      "4         63.472042\n",
      "           ...     \n",
      "440   -26066.080078\n",
      "441   -18543.093750\n",
      "442        1.769874\n",
      "443    -8122.338379\n",
      "444   -17862.373047\n",
      "Length: 444, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 60)           540         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           1830        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           620         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            42          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,032\n",
      "Trainable params: 3,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 68079240.0000  val loss: 77651328.0000\n",
      "26 epoch - loss: 62462344.0000  val loss: 68503648.0000\n",
      "51 epoch - loss: 61710452.0000  val loss: 67222160.0000\n",
      "76 epoch - loss: 61223500.0000  val loss: 66465100.0000\n",
      "101 epoch - loss: 60685572.0000  val loss: 65826012.0000\n",
      "126 epoch - loss: 59887956.0000  val loss: 64766880.0000\n",
      "151 epoch - loss: 58547944.0000  val loss: 63444292.0000\n",
      "176 epoch - loss: 56451692.0000  val loss: 61591352.0000\n",
      "201 epoch - loss: 53394892.0000  val loss: 59285436.0000\n",
      "226 epoch - loss: 49626456.0000  val loss: 57120784.0000\n",
      "251 epoch - loss: 46155724.0000  val loss: 60900576.0000\n",
      "Training is finished.\n",
      "\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training of the propensity score neural network:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - loss: 62.2077  val loss: 12.8453\n",
      "26 epoch - loss: 43.3580  val loss: 3.5010\n",
      "51 epoch - loss: 11.1316  val loss: 2.0803\n",
      "76 epoch - loss: 9.3839  val loss: 1.1886\n",
      "101 epoch - loss: 6.1703  val loss: 1.0012\n",
      "126 epoch - loss: 4.3406  val loss: 0.8980\n",
      "151 epoch - loss: 4.3053  val loss: 0.7474\n",
      "176 epoch - loss: 4.1298  val loss: 0.6797\n",
      "201 epoch - loss: 4.3506  val loss: 0.6875\n",
      "Training is finished.\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "144739.015625\n",
      "-0.001953125\n",
      "0       6519.608887\n",
      "1       4140.580566\n",
      "2       5406.432617\n",
      "3       5003.868164\n",
      "4       5582.833008\n",
      "           ...     \n",
      "440   -27418.722656\n",
      "441   -12471.554688\n",
      "442        2.434567\n",
      "443    -2049.058105\n",
      "444    -7189.387695\n",
      "Length: 444, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 80)           720         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 80)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 80)           6480        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 80)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 80)           6480        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            162         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,842\n",
      "Trainable params: 13,842\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 70228064.0000  val loss: 79588616.0000\n",
      "26 epoch - loss: 62202928.0000  val loss: 67595144.0000\n",
      "51 epoch - loss: 61510368.0000  val loss: 67193352.0000\n",
      "76 epoch - loss: 60926620.0000  val loss: 66363564.0000\n",
      "101 epoch - loss: 60092320.0000  val loss: 65291660.0000\n",
      "126 epoch - loss: 58519252.0000  val loss: 63220208.0000\n",
      "151 epoch - loss: 54987628.0000  val loss: 59350760.0000\n",
      "176 epoch - loss: 48074296.0000  val loss: 54014712.0000\n",
      "201 epoch - loss: 42575976.0000  val loss: 51777756.0000\n",
      "226 epoch - loss: 41696388.0000  val loss: 53314996.0000\n",
      "Training is finished.\n",
      "\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 62.2077  val loss: 12.8453\n",
      "26 epoch - loss: 43.3580  val loss: 3.5010\n",
      "51 epoch - loss: 11.1316  val loss: 2.0803\n",
      "76 epoch - loss: 9.3839  val loss: 1.1886\n",
      "101 epoch - loss: 6.1703  val loss: 1.0012\n",
      "126 epoch - loss: 4.3406  val loss: 0.8980\n",
      "151 epoch - loss: 4.3053  val loss: 0.7474\n",
      "176 epoch - loss: 4.1298  val loss: 0.6797\n",
      "201 epoch - loss: 4.3506  val loss: 0.6875\n",
      "Training is finished.\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "144739.03125\n",
      "-0.0009765625\n",
      "0      11659.552734\n",
      "1       7572.437988\n",
      "2       9874.829102\n",
      "3       9123.946289\n",
      "4       9941.075195\n",
      "           ...     \n",
      "440   -13741.007812\n",
      "441    -7604.104492\n",
      "442        1.515727\n",
      "443     6817.434082\n",
      "444    -2799.214111\n",
      "Length: 444, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           180         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           315         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           160         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            55          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5)            0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            12          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_4[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 722\n",
      "Trainable params: 722\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Training of the causal coefficients neural network:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - loss: 68740424.0000  val loss: 80939864.0000\n",
      "26 epoch - loss: 66497212.0000  val loss: 78886312.0000\n",
      "51 epoch - loss: 63737440.0000  val loss: 72953680.0000\n",
      "76 epoch - loss: 62052592.0000  val loss: 67803552.0000\n",
      "101 epoch - loss: 61739536.0000  val loss: 67904928.0000\n",
      "Training is finished.\n",
      "\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training of the propensity score neural network:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - loss: 70.8913  val loss: 49.0553\n",
      "26 epoch - loss: 58.3323  val loss: 28.0170\n",
      "51 epoch - loss: 42.9147  val loss: 14.0345\n",
      "76 epoch - loss: 20.3251  val loss: 6.8308\n",
      "101 epoch - loss: 10.4256  val loss: 2.7598\n",
      "126 epoch - loss: 26.8426  val loss: 1.3145\n",
      "151 epoch - loss: 5.2269  val loss: 0.8262\n",
      "176 epoch - loss: 5.7038  val loss: 0.7577\n",
      "201 epoch - loss: 5.0894  val loss: 0.6902\n",
      "226 epoch - loss: 3.2465  val loss: 0.6931\n",
      "Training is finished.\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "144087.046875\n",
      "-0.0009765625\n",
      "0         77.986259\n",
      "1         49.638145\n",
      "2         68.013527\n",
      "3         62.657520\n",
      "4         69.845398\n",
      "           ...     \n",
      "440   -17699.837891\n",
      "441   -13940.958984\n",
      "442    16994.837891\n",
      "443    -2644.371582\n",
      "444   -13016.760742\n",
      "Length: 442, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 60)           540         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           1830        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           620         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           210         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            22          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_4[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,222\n",
      "Trainable params: 3,222\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 70153328.0000  val loss: 82558160.0000\n",
      "26 epoch - loss: 66661516.0000  val loss: 79142552.0000\n",
      "51 epoch - loss: 63522492.0000  val loss: 74199896.0000\n",
      "76 epoch - loss: 62429508.0000  val loss: 69666424.0000\n",
      "101 epoch - loss: 61833308.0000  val loss: 69063752.0000\n",
      "126 epoch - loss: 61338784.0000  val loss: 67770472.0000\n",
      "151 epoch - loss: 60892184.0000  val loss: 66717592.0000\n",
      "176 epoch - loss: 60309124.0000  val loss: 65711484.0000\n",
      "201 epoch - loss: 59356644.0000  val loss: 64227104.0000\n",
      "226 epoch - loss: 57739712.0000  val loss: 61993052.0000\n",
      "251 epoch - loss: 54983588.0000  val loss: 58573248.0000\n",
      "276 epoch - loss: 51097912.0000  val loss: 54828804.0000\n",
      "301 epoch - loss: 47413288.0000  val loss: 52004272.0000\n",
      "326 epoch - loss: 45663524.0000  val loss: 50607972.0000\n",
      "351 epoch - loss: 45117280.0000  val loss: 49077036.0000\n",
      "376 epoch - loss: 44654536.0000  val loss: 48362040.0000\n",
      "401 epoch - loss: 44179816.0000  val loss: 47904088.0000\n",
      "426 epoch - loss: 43712252.0000  val loss: 47878784.0000\n",
      "451 epoch - loss: 43246604.0000  val loss: 47726752.0000\n",
      "476 epoch - loss: 42762704.0000  val loss: 47577604.0000\n",
      "Training is finished.\n",
      "\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 70.8913  val loss: 49.0553\n",
      "26 epoch - loss: 58.3323  val loss: 28.0170\n",
      "51 epoch - loss: 42.9147  val loss: 14.0345\n",
      "76 epoch - loss: 20.3251  val loss: 6.8308\n",
      "101 epoch - loss: 10.4256  val loss: 2.7598\n",
      "126 epoch - loss: 26.8426  val loss: 1.3145\n",
      "151 epoch - loss: 5.2269  val loss: 0.8262\n",
      "176 epoch - loss: 5.7038  val loss: 0.7577\n",
      "201 epoch - loss: 5.0894  val loss: 0.6902\n",
      "226 epoch - loss: 3.2465  val loss: 0.6931\n",
      "Training is finished.\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "144087.0625\n",
      "-0.0009765625\n",
      "0      11137.013672\n",
      "1       7362.188965\n",
      "2       9450.555664\n",
      "3       8760.613281\n",
      "4       9489.293945\n",
      "           ...     \n",
      "440   -26381.962891\n",
      "441    -9196.520508\n",
      "442    27548.417969\n",
      "443     3330.298584\n",
      "444    -3138.360352\n",
      "Length: 442, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "att1_1, se1_1=NSW_analyzer(Y,T,X,[20,15,5],[20,10,5])\n",
    "att1_2, se1_2=NSW_analyzer(Y,T,X,[60,30,20],[20,10,5])\n",
    "att1_3, se1_3=NSW_analyzer(Y,T,X,[80,80,80],[20,10,5])\n",
    "att1_4, se1_4=NSW_analyzer(Y,T,X,[20,15,10,5],[20,10,5])\n",
    "att1_5, se1_5=NSW_analyzer(Y,T,X,[60,30,20,10],[20,10,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "69cbab77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664.890625 875.7344144699382\n",
      "1264.4840087890625 800.4714688057649\n",
      "1595.4166259765625 768.4687160782747\n",
      "1199.5286865234375 858.0063631483279\n",
      "1569.9820556640625 746.6227183197132\n"
     ]
    }
   ],
   "source": [
    "print(att1_1,se1_1)\n",
    "print(att1_2,se1_2)\n",
    "print(att1_3,se1_3)\n",
    "print(att1_4,se1_4)\n",
    "print(att1_5,se1_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fde17e",
   "metadata": {},
   "source": [
    "### 2.PSID Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "9c6f00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSW_2 = pd.read_stata(\"NSW/psid_controls.dta\")\n",
    "NSW_2=NSW_2.append(NSW_1)\n",
    "NSW_2=NSW_2.drop(NSW_2[(NSW_2['data_id']!='PSID') & (NSW_2['treat']==0)].index)\n",
    "Y=NSW_2[\"re78\"]\n",
    "T=NSW_2[\"treat\"]\n",
    "X=NSW_2.iloc[:,2:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "69880ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           180         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           315         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 5)            80          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5)            0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            12          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 587\n",
      "Trainable params: 587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 710635904.0000  val loss: 664600192.0000\n",
      "26 epoch - loss: 595332352.0000  val loss: 548112192.0000\n",
      "51 epoch - loss: 310900352.0000  val loss: 283978400.0000\n",
      "76 epoch - loss: 142510320.0000  val loss: 145242224.0000\n",
      "101 epoch - loss: 109623224.0000  val loss: 136101392.0000\n",
      "Training is finished.\n",
      "\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 83.6518  val loss: 8.1462\n",
      "26 epoch - loss: 35.0919  val loss: 5.5029\n",
      "Training is finished.\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "787262.875\n",
      "-3.0517578125e-05\n",
      "0         -33.989017\n",
      "1         -28.253307\n",
      "2         -46.102009\n",
      "3        -201.773941\n",
      "4         -16.982578\n",
      "           ...      \n",
      "180    175383.234375\n",
      "181    272104.718750\n",
      "182    175504.578125\n",
      "183    220984.390625\n",
      "184    306545.968750\n",
      "Length: 2415, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 60)           540         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           1830        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           620         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            42          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,032\n",
      "Trainable params: 3,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Training of the causal coefficients neural network:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - loss: 659835200.0000  val loss: 596549248.0000\n",
      "26 epoch - loss: 296985888.0000  val loss: 262850400.0000\n",
      "51 epoch - loss: 119283864.0000  val loss: 149512656.0000\n",
      "Training is finished.\n",
      "\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training of the propensity score neural network:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - loss: 83.6518  val loss: 8.1462\n",
      "26 epoch - loss: 35.0919  val loss: 5.5029\n",
      "Training is finished.\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "787263.0625\n",
      "-0.00390625\n",
      "0         -48.397648\n",
      "1         -41.474361\n",
      "2         -65.524017\n",
      "3        -290.222321\n",
      "4         -24.988577\n",
      "           ...      \n",
      "180    197041.375000\n",
      "181    315897.750000\n",
      "182    172287.781250\n",
      "183    211563.062500\n",
      "184    289138.187500\n",
      "Length: 2415, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 80)           720         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 80)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 80)           6480        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 80)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 80)           6480        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            162         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,842\n",
      "Trainable params: 13,842\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 703750656.0000  val loss: 607447936.0000\n",
      "26 epoch - loss: 129027720.0000  val loss: 160824048.0000\n",
      "51 epoch - loss: 108913912.0000  val loss: 124586392.0000\n",
      "Training is finished.\n",
      "\n",
      "76/76 [==============================] - 0s 3ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 83.6518  val loss: 8.1462\n",
      "26 epoch - loss: 35.0919  val loss: 5.5029\n",
      "Training is finished.\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "787263.0\n",
      "-0.0078125\n",
      "0         -28.697567\n",
      "1         -24.229412\n",
      "2         -38.680939\n",
      "3        -161.469421\n",
      "4         -14.736248\n",
      "           ...      \n",
      "180    202492.718750\n",
      "181    314562.093750\n",
      "182    176120.875000\n",
      "183    205949.546875\n",
      "184    266360.875000\n",
      "Length: 2415, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           180         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           315         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           160         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            55          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5)            0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            12          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_4[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 722\n",
      "Trainable params: 722\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 747965056.0000  val loss: 691849280.0000\n",
      "26 epoch - loss: 461016640.0000  val loss: 416027648.0000\n",
      "51 epoch - loss: 181797584.0000  val loss: 165725296.0000\n",
      "76 epoch - loss: 112411384.0000  val loss: 132898904.0000\n",
      "Training is finished.\n",
      "\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training of the propensity score neural network:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - loss: 1625.5573  val loss: 389.3723\n",
      "26 epoch - loss: 585.3481  val loss: 9.7186\n",
      "Training is finished.\n",
      "76/76 [==============================] - 0s 3ms/step\n",
      "787262.9375\n",
      "-0.00048828125\n",
      "0     -4.029906e+06\n",
      "1     -4.162184e+06\n",
      "2     -3.819260e+06\n",
      "3     -2.615975e+06\n",
      "4     -4.388345e+06\n",
      "           ...     \n",
      "180    2.026125e+05\n",
      "181    3.120962e+05\n",
      "182    1.759383e+05\n",
      "183    2.082203e+05\n",
      "184    2.673050e+05\n",
      "Length: 2415, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 60)           540         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           1830        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           620         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           210         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            22          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_4[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,222\n",
      "Trainable params: 3,222\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 760895872.0000  val loss: 706307776.0000\n",
      "26 epoch - loss: 663515072.0000  val loss: 617916224.0000\n",
      "51 epoch - loss: 580501760.0000  val loss: 535676960.0000\n",
      "76 epoch - loss: 368564768.0000  val loss: 329675360.0000\n",
      "101 epoch - loss: 114479520.0000  val loss: 130419472.0000\n",
      "126 epoch - loss: 113897368.0000  val loss: 128032088.0000\n",
      "Training is finished.\n",
      "\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 1625.5573  val loss: 389.3723\n",
      "26 epoch - loss: 585.3481  val loss: 9.7186\n",
      "Training is finished.\n",
      "76/76 [==============================] - 0s 2ms/step\n",
      "787263.0\n",
      "-0.015625\n",
      "0     -4.456982e+06\n",
      "1     -4.830634e+06\n",
      "2     -4.211924e+06\n",
      "3     -3.015952e+06\n",
      "4     -5.102099e+06\n",
      "           ...     \n",
      "180    1.854822e+05\n",
      "181    2.760081e+05\n",
      "182    1.734600e+05\n",
      "183    2.066946e+05\n",
      "184    2.708413e+05\n",
      "Length: 2415, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "att2_1, se2_1=NSW_analyzer(Y,T,X,[20,15,5],[20,10,5])\n",
    "att2_2, se2_2=NSW_analyzer(Y,T,X,[60,30,20],[20,10,5])\n",
    "att2_3, se2_3=NSW_analyzer(Y,T,X,[80,80,80],[20,10,5])\n",
    "att2_4, se2_4=NSW_analyzer(Y,T,X,[20,15,10,5],[20,10,5])\n",
    "att2_5, se2_5=NSW_analyzer(Y,T,X,[60,30,20,10],[20,10,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "a7983898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3677.775634765625 737.1709812949512\n",
      "3593.18310546875 737.7384459927857\n",
      "3657.72265625 737.818172437936\n",
      "-32875852.0 10903932.39759428\n",
      "-38224084.0 10975754.654550781\n"
     ]
    }
   ],
   "source": [
    "print(att2_1,se2_1)\n",
    "print(att2_2,se2_2)\n",
    "print(att2_3,se2_3)\n",
    "print(att2_4,se2_4)\n",
    "print(att2_5,se2_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af36f0",
   "metadata": {},
   "source": [
    "### 3. CPS controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "5f7dedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSW_3 = pd.read_stata(\"NSW/cps_controls.dta\")\n",
    "NSW_3 = NSW_3.append(NSW_1)\n",
    "###dropping original sample control variables\n",
    "NSW_3=NSW_3.drop(NSW_3[(NSW_3['data_id']!='CPS1') & (NSW_3['treat']==0)].index)\n",
    "Y=NSW_3[\"re78\"]\n",
    "T=NSW_3[\"treat\"]\n",
    "X=NSW_3.iloc[:,2:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "ba73062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           180         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           315         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 5)            80          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5)            0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            12          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 587\n",
      "Trainable params: 587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 308795712.0000  val loss: 320238176.0000\n",
      "26 epoch - loss: 258361264.0000  val loss: 262748640.0000\n",
      "51 epoch - loss: 131410368.0000  val loss: 133008088.0000\n",
      "76 epoch - loss: 64115716.0000  val loss: 65034708.0000\n",
      "101 epoch - loss: 56411276.0000  val loss: 57378284.0000\n",
      "126 epoch - loss: 55637892.0000  val loss: 56967180.0000\n",
      "151 epoch - loss: 55527944.0000  val loss: 56887552.0000\n",
      "176 epoch - loss: 55451824.0000  val loss: 56813544.0000\n",
      "201 epoch - loss: 55405128.0000  val loss: 56777728.0000\n",
      "226 epoch - loss: 55374324.0000  val loss: 56754012.0000\n",
      "251 epoch - loss: 55344744.0000  val loss: 56725140.0000\n",
      "276 epoch - loss: 55319756.0000  val loss: 56697608.0000\n",
      "301 epoch - loss: 55297864.0000  val loss: 56677092.0000\n",
      "326 epoch - loss: 55277988.0000  val loss: 56662500.0000\n",
      "351 epoch - loss: 55261152.0000  val loss: 56649380.0000\n",
      "376 epoch - loss: 55246260.0000  val loss: 56636728.0000\n",
      "401 epoch - loss: 55230364.0000  val loss: 56621032.0000\n",
      "426 epoch - loss: 55192080.0000  val loss: 56591768.0000\n",
      "451 epoch - loss: 55167836.0000  val loss: 56555676.0000\n",
      "476 epoch - loss: 55144696.0000  val loss: 56530956.0000\n",
      "501 epoch - loss: 55122344.0000  val loss: 56505528.0000\n",
      "526 epoch - loss: 55099896.0000  val loss: 56480700.0000\n",
      "551 epoch - loss: 55072468.0000  val loss: 56450568.0000\n",
      "576 epoch - loss: 55047704.0000  val loss: 56423376.0000\n",
      "601 epoch - loss: 55021292.0000  val loss: 56399048.0000\n",
      "626 epoch - loss: 54995828.0000  val loss: 56372796.0000\n",
      "651 epoch - loss: 54971452.0000  val loss: 56329520.0000\n",
      "676 epoch - loss: 54945180.0000  val loss: 56310084.0000\n",
      "701 epoch - loss: 54918920.0000  val loss: 56275820.0000\n",
      "726 epoch - loss: 54892208.0000  val loss: 56239340.0000\n",
      "751 epoch - loss: 54864424.0000  val loss: 56208240.0000\n",
      "776 epoch - loss: 54835736.0000  val loss: 56164896.0000\n",
      "801 epoch - loss: 54796000.0000  val loss: 56114180.0000\n",
      "826 epoch - loss: 54651588.0000  val loss: 55962444.0000\n",
      "851 epoch - loss: 54587652.0000  val loss: 55870080.0000\n",
      "876 epoch - loss: 54527212.0000  val loss: 55793524.0000\n",
      "901 epoch - loss: 54473792.0000  val loss: 55738452.0000\n",
      "926 epoch - loss: 54421228.0000  val loss: 55665060.0000\n",
      "951 epoch - loss: 54368228.0000  val loss: 55608440.0000\n",
      "976 epoch - loss: 54315260.0000  val loss: 55545172.0000\n",
      "1001 epoch - loss: 54261660.0000  val loss: 55471748.0000\n",
      "1026 epoch - loss: 54207516.0000  val loss: 55404308.0000\n",
      "1051 epoch - loss: 54151888.0000  val loss: 55341252.0000\n",
      "1076 epoch - loss: 54095872.0000  val loss: 55271132.0000\n",
      "1101 epoch - loss: 54039040.0000  val loss: 55192176.0000\n",
      "1126 epoch - loss: 53980248.0000  val loss: 55126336.0000\n",
      "1151 epoch - loss: 53921608.0000  val loss: 55046608.0000\n",
      "1176 epoch - loss: 53863064.0000  val loss: 54976848.0000\n",
      "1201 epoch - loss: 53808552.0000  val loss: 54892380.0000\n",
      "1226 epoch - loss: 53744028.0000  val loss: 54816296.0000\n",
      "1251 epoch - loss: 53685260.0000  val loss: 54739728.0000\n",
      "1276 epoch - loss: 53624268.0000  val loss: 54664252.0000\n",
      "1301 epoch - loss: 53565272.0000  val loss: 54583980.0000\n",
      "1326 epoch - loss: 53502260.0000  val loss: 54507120.0000\n",
      "1351 epoch - loss: 53441416.0000  val loss: 54428400.0000\n",
      "1376 epoch - loss: 53373548.0000  val loss: 54357084.0000\n",
      "1401 epoch - loss: 53312004.0000  val loss: 54272792.0000\n",
      "1426 epoch - loss: 53254596.0000  val loss: 54190512.0000\n",
      "1451 epoch - loss: 53198468.0000  val loss: 54114140.0000\n",
      "1476 epoch - loss: 53146604.0000  val loss: 54040076.0000\n",
      "1501 epoch - loss: 53091000.0000  val loss: 53955640.0000\n",
      "1526 epoch - loss: 53041048.0000  val loss: 53886744.0000\n",
      "1551 epoch - loss: 52993984.0000  val loss: 53824976.0000\n",
      "1576 epoch - loss: 52948528.0000  val loss: 53736320.0000\n",
      "1601 epoch - loss: 52904676.0000  val loss: 53691660.0000\n",
      "1626 epoch - loss: 52864060.0000  val loss: 53616428.0000\n",
      "1651 epoch - loss: 52822656.0000  val loss: 53570348.0000\n",
      "1676 epoch - loss: 52784428.0000  val loss: 53521716.0000\n",
      "1701 epoch - loss: 52747508.0000  val loss: 53470512.0000\n",
      "1726 epoch - loss: 52711856.0000  val loss: 53423820.0000\n",
      "1751 epoch - loss: 52678016.0000  val loss: 53372032.0000\n",
      "1776 epoch - loss: 52646164.0000  val loss: 53319712.0000\n",
      "1801 epoch - loss: 52613512.0000  val loss: 53296628.0000\n",
      "1826 epoch - loss: 52581324.0000  val loss: 53249692.0000\n",
      "1851 epoch - loss: 52551556.0000  val loss: 53218092.0000\n",
      "1876 epoch - loss: 52522216.0000  val loss: 53173872.0000\n",
      "1901 epoch - loss: 52493104.0000  val loss: 53148008.0000\n",
      "1926 epoch - loss: 52463152.0000  val loss: 53111248.0000\n",
      "1951 epoch - loss: 52434652.0000  val loss: 53084300.0000\n",
      "1976 epoch - loss: 52405360.0000  val loss: 53034952.0000\n",
      "2001 epoch - loss: 52378180.0000  val loss: 53005820.0000\n",
      "2026 epoch - loss: 52349292.0000  val loss: 52963380.0000\n",
      "2051 epoch - loss: 52321344.0000  val loss: 52931296.0000\n",
      "2076 epoch - loss: 52292992.0000  val loss: 52898176.0000\n",
      "2101 epoch - loss: 52265868.0000  val loss: 52897880.0000\n",
      "2126 epoch - loss: 52237396.0000  val loss: 52829980.0000\n",
      "2151 epoch - loss: 52208316.0000  val loss: 52818712.0000\n",
      "2176 epoch - loss: 52181048.0000  val loss: 52762260.0000\n",
      "2201 epoch - loss: 52152720.0000  val loss: 52760540.0000\n",
      "2226 epoch - loss: 52120608.0000  val loss: 52719748.0000\n",
      "2251 epoch - loss: 52091056.0000  val loss: 52685148.0000\n",
      "2276 epoch - loss: 52061504.0000  val loss: 52664524.0000\n",
      "2301 epoch - loss: 52031564.0000  val loss: 52618380.0000\n",
      "2326 epoch - loss: 52000100.0000  val loss: 52582820.0000\n",
      "2351 epoch - loss: 51975596.0000  val loss: 52533680.0000\n",
      "2376 epoch - loss: 51937776.0000  val loss: 52534144.0000\n",
      "2401 epoch - loss: 51906324.0000  val loss: 52508704.0000\n",
      "2426 epoch - loss: 51873588.0000  val loss: 52464976.0000\n",
      "2451 epoch - loss: 51844232.0000  val loss: 52432236.0000\n",
      "2476 epoch - loss: 51811860.0000  val loss: 52411084.0000\n",
      "2501 epoch - loss: 51775920.0000  val loss: 52372480.0000\n",
      "2526 epoch - loss: 51741240.0000  val loss: 52356400.0000\n",
      "2551 epoch - loss: 51711728.0000  val loss: 52285532.0000\n",
      "2576 epoch - loss: 51671552.0000  val loss: 52257992.0000\n",
      "2601 epoch - loss: 51633844.0000  val loss: 52237452.0000\n",
      "2626 epoch - loss: 51600916.0000  val loss: 52184320.0000\n",
      "2651 epoch - loss: 51559576.0000  val loss: 52167088.0000\n",
      "2676 epoch - loss: 51522288.0000  val loss: 52139308.0000\n",
      "2701 epoch - loss: 51485308.0000  val loss: 52086804.0000\n",
      "2726 epoch - loss: 51445120.0000  val loss: 52056528.0000\n",
      "2751 epoch - loss: 51407528.0000  val loss: 52025020.0000\n",
      "2776 epoch - loss: 51366544.0000  val loss: 51966744.0000\n",
      "2801 epoch - loss: 51321748.0000  val loss: 51957988.0000\n",
      "2826 epoch - loss: 51279452.0000  val loss: 51891440.0000\n",
      "2851 epoch - loss: 51232248.0000  val loss: 51862924.0000\n",
      "2876 epoch - loss: 51183848.0000  val loss: 51833388.0000\n",
      "2901 epoch - loss: 51141664.0000  val loss: 51783268.0000\n",
      "2926 epoch - loss: 51093208.0000  val loss: 51748124.0000\n",
      "2951 epoch - loss: 51045848.0000  val loss: 51749584.0000\n",
      "2976 epoch - loss: 50999004.0000  val loss: 51627992.0000\n",
      "3001 epoch - loss: 50949300.0000  val loss: 51622032.0000\n",
      "Training is finished.\n",
      "\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 58.8770  val loss: 1.4714\n",
      "26 epoch - loss: 17.1346  val loss: 0.9225\n",
      "51 epoch - loss: 8.8178  val loss: 1.0944\n",
      "Training is finished.\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "5202870.0\n",
      "-0.0625\n",
      "0      2.029382e+01\n",
      "1      4.785501e+01\n",
      "2      2.277461e+01\n",
      "3      2.334317e+01\n",
      "4     -4.643265e+00\n",
      "           ...     \n",
      "180    1.360319e+06\n",
      "181    2.124450e+06\n",
      "182    1.195394e+06\n",
      "183    1.456194e+06\n",
      "184    1.973811e+06\n",
      "Length: 15874, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 60)           540         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           1830        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           620         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            42          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,032\n",
      "Trainable params: 3,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 284867360.0000  val loss: 287393952.0000\n",
      "26 epoch - loss: 125655440.0000  val loss: 124009072.0000\n",
      "51 epoch - loss: 60378208.0000  val loss: 60313012.0000\n",
      "76 epoch - loss: 55532116.0000  val loss: 56876924.0000\n",
      "101 epoch - loss: 55375288.0000  val loss: 56725884.0000\n",
      "126 epoch - loss: 55311148.0000  val loss: 56684220.0000\n",
      "151 epoch - loss: 55266196.0000  val loss: 56624756.0000\n",
      "176 epoch - loss: 55210100.0000  val loss: 56549596.0000\n",
      "201 epoch - loss: 55133204.0000  val loss: 56466272.0000\n",
      "226 epoch - loss: 55048388.0000  val loss: 56358048.0000\n",
      "251 epoch - loss: 54942272.0000  val loss: 56235532.0000\n",
      "276 epoch - loss: 54757040.0000  val loss: 56018276.0000\n",
      "301 epoch - loss: 54567860.0000  val loss: 55787676.0000\n",
      "326 epoch - loss: 54338144.0000  val loss: 55470648.0000\n",
      "351 epoch - loss: 53962732.0000  val loss: 55042428.0000\n",
      "376 epoch - loss: 53575456.0000  val loss: 54529820.0000\n",
      "401 epoch - loss: 53182192.0000  val loss: 53946816.0000\n",
      "426 epoch - loss: 52867640.0000  val loss: 53603784.0000\n",
      "451 epoch - loss: 52626380.0000  val loss: 53299788.0000\n",
      "476 epoch - loss: 52418460.0000  val loss: 53123876.0000\n",
      "501 epoch - loss: 52225568.0000  val loss: 52936016.0000\n",
      "526 epoch - loss: 52043176.0000  val loss: 52700460.0000\n",
      "551 epoch - loss: 51881172.0000  val loss: 52540328.0000\n",
      "576 epoch - loss: 51728176.0000  val loss: 52391176.0000\n",
      "601 epoch - loss: 51581192.0000  val loss: 52252128.0000\n",
      "626 epoch - loss: 51436420.0000  val loss: 52141952.0000\n",
      "651 epoch - loss: 51290620.0000  val loss: 52045628.0000\n",
      "676 epoch - loss: 51191196.0000  val loss: 52141852.0000\n",
      "701 epoch - loss: 51002984.0000  val loss: 51668160.0000\n",
      "726 epoch - loss: 50835564.0000  val loss: 51535804.0000\n",
      "751 epoch - loss: 50673004.0000  val loss: 51413004.0000\n",
      "776 epoch - loss: 50963852.0000  val loss: 51395332.0000\n",
      "801 epoch - loss: 50716756.0000  val loss: 51341180.0000\n",
      "Training is finished.\n",
      "\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 58.8770  val loss: 1.4714\n",
      "26 epoch - loss: 17.1346  val loss: 0.9225\n",
      "51 epoch - loss: 8.8178  val loss: 1.0944\n",
      "Training is finished.\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "5202870.0\n",
      "-0.0625\n",
      "0      1.672117e+01\n",
      "1      3.877436e+01\n",
      "2      1.894122e+01\n",
      "3      1.845704e+01\n",
      "4      6.204063e+00\n",
      "           ...     \n",
      "180    1.357003e+06\n",
      "181    2.107093e+06\n",
      "182    1.191609e+06\n",
      "183    1.451235e+06\n",
      "184    1.940403e+06\n",
      "Length: 15874, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 80)           720         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 80)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 80)           6480        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 80)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 80)           6480        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            162         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,842\n",
      "Trainable params: 13,842\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 305181280.0000  val loss: 292444384.0000\n",
      "26 epoch - loss: 65965060.0000  val loss: 65565464.0000\n",
      "51 epoch - loss: 55960460.0000  val loss: 57159248.0000\n",
      "76 epoch - loss: 55360560.0000  val loss: 56810904.0000\n",
      "101 epoch - loss: 55293432.0000  val loss: 56702952.0000\n",
      "126 epoch - loss: 55223980.0000  val loss: 56606432.0000\n",
      "151 epoch - loss: 55127876.0000  val loss: 56479004.0000\n",
      "176 epoch - loss: 54982772.0000  val loss: 56307472.0000\n",
      "201 epoch - loss: 54673792.0000  val loss: 56201124.0000\n",
      "226 epoch - loss: 54312880.0000  val loss: 55526240.0000\n",
      "251 epoch - loss: 53905972.0000  val loss: 55062272.0000\n",
      "276 epoch - loss: 53389324.0000  val loss: 54238780.0000\n",
      "301 epoch - loss: 53007008.0000  val loss: 53918516.0000\n",
      "326 epoch - loss: 52717760.0000  val loss: 53664536.0000\n",
      "351 epoch - loss: 52361068.0000  val loss: 52998276.0000\n",
      "376 epoch - loss: 52316308.0000  val loss: 52776308.0000\n",
      "401 epoch - loss: 52031156.0000  val loss: 52775848.0000\n",
      "426 epoch - loss: 51839372.0000  val loss: 52464460.0000\n",
      "451 epoch - loss: 51678124.0000  val loss: 52341020.0000\n",
      "476 epoch - loss: 51542740.0000  val loss: 52313164.0000\n",
      "501 epoch - loss: 51376004.0000  val loss: 51950576.0000\n",
      "526 epoch - loss: 51193708.0000  val loss: 51792160.0000\n",
      "551 epoch - loss: 51709472.0000  val loss: 52500116.0000\n",
      "576 epoch - loss: 50927936.0000  val loss: 51661764.0000\n",
      "601 epoch - loss: 51596720.0000  val loss: 52360804.0000\n",
      "626 epoch - loss: 50786904.0000  val loss: 52055940.0000\n",
      "651 epoch - loss: 50531608.0000  val loss: 51086516.0000\n",
      "676 epoch - loss: 51208028.0000  val loss: 51988172.0000\n",
      "Training is finished.\n",
      "\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 58.8770  val loss: 1.4714\n",
      "26 epoch - loss: 17.1346  val loss: 0.9225\n",
      "51 epoch - loss: 8.8178  val loss: 1.0944\n",
      "Training is finished.\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "5202870.0\n",
      "-0.0625\n",
      "0      1.999266e+01\n",
      "1      3.663757e+01\n",
      "2      1.918783e+01\n",
      "3      1.793231e+01\n",
      "4     -3.528151e+00\n",
      "           ...     \n",
      "180    1.361355e+06\n",
      "181    2.060104e+06\n",
      "182    1.183362e+06\n",
      "183    1.419324e+06\n",
      "184    1.973889e+06\n",
      "Length: 15874, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           180         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 15)           315         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           160         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            55          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5)            0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            12          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_4[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 722\n",
      "Trainable params: 722\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 325601824.0000  val loss: 333153312.0000\n",
      "26 epoch - loss: 196700112.0000  val loss: 199252528.0000\n",
      "51 epoch - loss: 74116248.0000  val loss: 74474632.0000\n",
      "76 epoch - loss: 57356528.0000  val loss: 58187860.0000\n",
      "101 epoch - loss: 55797076.0000  val loss: 57120552.0000\n",
      "126 epoch - loss: 55537472.0000  val loss: 56843000.0000\n",
      "151 epoch - loss: 55444768.0000  val loss: 56778872.0000\n",
      "176 epoch - loss: 55387200.0000  val loss: 56724656.0000\n",
      "201 epoch - loss: 55345552.0000  val loss: 56693620.0000\n",
      "226 epoch - loss: 55313816.0000  val loss: 56670568.0000\n",
      "251 epoch - loss: 55284580.0000  val loss: 56643388.0000\n",
      "276 epoch - loss: 55256240.0000  val loss: 56608468.0000\n",
      "301 epoch - loss: 55226648.0000  val loss: 56570372.0000\n",
      "326 epoch - loss: 55194572.0000  val loss: 56526356.0000\n",
      "351 epoch - loss: 55157944.0000  val loss: 56476636.0000\n",
      "376 epoch - loss: 55104120.0000  val loss: 56384560.0000\n",
      "401 epoch - loss: 55045352.0000  val loss: 56309880.0000\n",
      "426 epoch - loss: 54976584.0000  val loss: 56214400.0000\n",
      "451 epoch - loss: 54882744.0000  val loss: 56087784.0000\n",
      "476 epoch - loss: 54782140.0000  val loss: 55939240.0000\n",
      "501 epoch - loss: 54665300.0000  val loss: 55766928.0000\n",
      "526 epoch - loss: 54541432.0000  val loss: 55607564.0000\n",
      "551 epoch - loss: 54411696.0000  val loss: 55406356.0000\n",
      "576 epoch - loss: 54221592.0000  val loss: 55149016.0000\n",
      "601 epoch - loss: 54093680.0000  val loss: 54934624.0000\n",
      "626 epoch - loss: 53938636.0000  val loss: 54703376.0000\n",
      "651 epoch - loss: 53646464.0000  val loss: 54261280.0000\n",
      "676 epoch - loss: 53298900.0000  val loss: 54014328.0000\n",
      "701 epoch - loss: 53069232.0000  val loss: 53553624.0000\n",
      "726 epoch - loss: 52907980.0000  val loss: 53385156.0000\n",
      "751 epoch - loss: 52914708.0000  val loss: 53342516.0000\n",
      "776 epoch - loss: 52507564.0000  val loss: 52929368.0000\n",
      "801 epoch - loss: 52382004.0000  val loss: 52799888.0000\n",
      "826 epoch - loss: 52274236.0000  val loss: 52675464.0000\n",
      "851 epoch - loss: 52158968.0000  val loss: 52560388.0000\n",
      "876 epoch - loss: 52054380.0000  val loss: 52499420.0000\n",
      "901 epoch - loss: 51939104.0000  val loss: 52428352.0000\n",
      "926 epoch - loss: 51828512.0000  val loss: 52353384.0000\n",
      "951 epoch - loss: 51699180.0000  val loss: 52194160.0000\n",
      "976 epoch - loss: 51658736.0000  val loss: 52125656.0000\n",
      "1001 epoch - loss: 51558320.0000  val loss: 51980608.0000\n",
      "1026 epoch - loss: 51329836.0000  val loss: 51911072.0000\n",
      "1051 epoch - loss: 51223900.0000  val loss: 51771308.0000\n",
      "1076 epoch - loss: 51117328.0000  val loss: 51648904.0000\n",
      "1101 epoch - loss: 51300412.0000  val loss: 51590600.0000\n",
      "Training is finished.\n",
      "\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 1125.3542  val loss: 283.2949\n",
      "26 epoch - loss: 434.5020  val loss: 2.0987\n",
      "Training is finished.\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "5188763.5\n",
      "-0.03125\n",
      "0      1.631435e+01\n",
      "1      4.406907e+01\n",
      "2      2.066145e+01\n",
      "3      2.032607e+01\n",
      "4      1.976984e+00\n",
      "           ...     \n",
      "180    1.338494e+06\n",
      "181    2.101658e+06\n",
      "182    1.240343e+06\n",
      "183    1.480814e+06\n",
      "184    1.935749e+06\n",
      "Length: 15917, dtype: float32\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 60)           540         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           1830        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           620         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           210         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            22          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_4[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,222\n",
      "Trainable params: 3,222\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 331683872.0000  val loss: 340572928.0000\n",
      "26 epoch - loss: 287197408.0000  val loss: 297278272.0000\n",
      "51 epoch - loss: 249265360.0000  val loss: 256391696.0000\n",
      "76 epoch - loss: 145256656.0000  val loss: 145712160.0000\n",
      "101 epoch - loss: 58780356.0000  val loss: 60197080.0000\n",
      "126 epoch - loss: 57181908.0000  val loss: 58716556.0000\n",
      "151 epoch - loss: 56402008.0000  val loss: 57809916.0000\n",
      "176 epoch - loss: 55962268.0000  val loss: 57457696.0000\n",
      "201 epoch - loss: 55792620.0000  val loss: 57371956.0000\n",
      "226 epoch - loss: 55703592.0000  val loss: 57257516.0000\n",
      "251 epoch - loss: 55627864.0000  val loss: 57161428.0000\n",
      "276 epoch - loss: 55552448.0000  val loss: 57054976.0000\n",
      "301 epoch - loss: 55472900.0000  val loss: 56970452.0000\n",
      "326 epoch - loss: 55404204.0000  val loss: 56883240.0000\n",
      "351 epoch - loss: 55337956.0000  val loss: 56798644.0000\n",
      "376 epoch - loss: 55271680.0000  val loss: 56713124.0000\n",
      "401 epoch - loss: 55204752.0000  val loss: 56626748.0000\n",
      "426 epoch - loss: 55135832.0000  val loss: 56532036.0000\n",
      "451 epoch - loss: 55058924.0000  val loss: 56437240.0000\n",
      "476 epoch - loss: 54978696.0000  val loss: 56336876.0000\n",
      "501 epoch - loss: 54859660.0000  val loss: 56175440.0000\n",
      "526 epoch - loss: 54704228.0000  val loss: 55980836.0000\n",
      "551 epoch - loss: 54578256.0000  val loss: 55831804.0000\n",
      "576 epoch - loss: 54438564.0000  val loss: 55652076.0000\n",
      "601 epoch - loss: 54272992.0000  val loss: 55454448.0000\n",
      "626 epoch - loss: 54063356.0000  val loss: 55219924.0000\n",
      "651 epoch - loss: 53841328.0000  val loss: 54905528.0000\n",
      "676 epoch - loss: 53625408.0000  val loss: 54641612.0000\n",
      "701 epoch - loss: 53375260.0000  val loss: 54365292.0000\n",
      "726 epoch - loss: 53145420.0000  val loss: 54074004.0000\n",
      "751 epoch - loss: 52927244.0000  val loss: 53785228.0000\n",
      "776 epoch - loss: 52748776.0000  val loss: 53511632.0000\n",
      "801 epoch - loss: 52612912.0000  val loss: 53345280.0000\n",
      "826 epoch - loss: 52565076.0000  val loss: 53426016.0000\n",
      "851 epoch - loss: 52404980.0000  val loss: 53133136.0000\n",
      "876 epoch - loss: 52303952.0000  val loss: 53005832.0000\n",
      "901 epoch - loss: 52216140.0000  val loss: 52914444.0000\n",
      "926 epoch - loss: 52126704.0000  val loss: 52817636.0000\n",
      "951 epoch - loss: 52036408.0000  val loss: 52723712.0000\n",
      "976 epoch - loss: 51942416.0000  val loss: 52627260.0000\n",
      "1001 epoch - loss: 51847452.0000  val loss: 52537536.0000\n",
      "1026 epoch - loss: 51746736.0000  val loss: 52431980.0000\n",
      "1051 epoch - loss: 51653140.0000  val loss: 52352808.0000\n",
      "1076 epoch - loss: 51546904.0000  val loss: 52216972.0000\n",
      "1101 epoch - loss: 51443348.0000  val loss: 52119644.0000\n",
      "1126 epoch - loss: 51361128.0000  val loss: 52031312.0000\n",
      "1151 epoch - loss: 51264700.0000  val loss: 52040812.0000\n",
      "1176 epoch - loss: 51122940.0000  val loss: 51786164.0000\n",
      "1201 epoch - loss: 51070984.0000  val loss: 51631680.0000\n",
      "1226 epoch - loss: 50895388.0000  val loss: 51576672.0000\n",
      "1251 epoch - loss: 50798536.0000  val loss: 51477644.0000\n",
      "1276 epoch - loss: 50690380.0000  val loss: 51351520.0000\n",
      "1301 epoch - loss: 50635316.0000  val loss: 51237048.0000\n",
      "1326 epoch - loss: 50463468.0000  val loss: 51121404.0000\n",
      "1351 epoch - loss: 50504452.0000  val loss: 51751628.0000\n",
      "1376 epoch - loss: 50255096.0000  val loss: 51269488.0000\n",
      "1401 epoch - loss: 50254372.0000  val loss: 50974152.0000\n",
      "1426 epoch - loss: 50015512.0000  val loss: 50918548.0000\n",
      "1451 epoch - loss: 49920764.0000  val loss: 50920784.0000\n",
      "1476 epoch - loss: 49831684.0000  val loss: 50792160.0000\n",
      "1501 epoch - loss: 49646280.0000  val loss: 50600328.0000\n",
      "1526 epoch - loss: 49666508.0000  val loss: 50416204.0000\n",
      "1551 epoch - loss: 49636760.0000  val loss: 50913444.0000\n",
      "1576 epoch - loss: 49595732.0000  val loss: 50478612.0000\n",
      "1601 epoch - loss: 49335588.0000  val loss: 50264444.0000\n",
      "1626 epoch - loss: 49406796.0000  val loss: 50395516.0000\n",
      "1651 epoch - loss: 49257476.0000  val loss: 50604672.0000\n",
      "1676 epoch - loss: 49161976.0000  val loss: 50160296.0000\n",
      "1701 epoch - loss: 49159384.0000  val loss: 50016144.0000\n",
      "1726 epoch - loss: 49073884.0000  val loss: 50002716.0000\n",
      "1751 epoch - loss: 48885420.0000  val loss: 50145868.0000\n",
      "1776 epoch - loss: 48892356.0000  val loss: 49909216.0000\n",
      "1801 epoch - loss: 48738856.0000  val loss: 49794168.0000\n",
      "1826 epoch - loss: 48686160.0000  val loss: 49784204.0000\n",
      "1851 epoch - loss: 48640828.0000  val loss: 49804648.0000\n",
      "1876 epoch - loss: 48594812.0000  val loss: 49784604.0000\n",
      "1901 epoch - loss: 48560276.0000  val loss: 49727512.0000\n",
      "1926 epoch - loss: 48520368.0000  val loss: 49803048.0000\n",
      "1951 epoch - loss: 48650704.0000  val loss: 50192116.0000\n",
      "Training is finished.\n",
      "\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                180       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451\n",
      "Trainable params: 451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the propensity score neural network:\n",
      "1 epoch - loss: 1125.3542  val loss: 283.2949\n",
      "26 epoch - loss: 434.5020  val loss: 2.0987\n",
      "Training is finished.\n",
      "498/498 [==============================] - 1s 2ms/step\n",
      "5188764.0\n",
      "-0.0625\n",
      "0      3.079069e+01\n",
      "1      2.614427e+01\n",
      "2      2.608661e+01\n",
      "3      3.126381e+01\n",
      "4      8.798021e+00\n",
      "           ...     \n",
      "180    1.344714e+06\n",
      "181    2.077417e+06\n",
      "182    1.173404e+06\n",
      "183    1.304160e+06\n",
      "184    1.831812e+06\n",
      "Length: 15917, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "att3_1, se3_1=NSW_analyzer(Y,T,X,[20,15,5],[20,10,5])\n",
    "att3_2, se3_2=NSW_analyzer(Y,T,X,[60,30,20],[20,10,5])\n",
    "att3_3, se3_3=NSW_analyzer(Y,T,X,[80,80,80],[20,10,5])\n",
    "att3_4, se3_4=NSW_analyzer(Y,T,X,[20,15,10,5],[20,10,5])\n",
    "att3_5, se3_5=NSW_analyzer(Y,T,X,[60,30,20,10],[20,10,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a0c5e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-755.9854125976562 738.2211038268383\n",
      "-1397.52587890625 748.5120546346571\n",
      "-841.8257446289062 737.3540477227558\n",
      "79584328.0 8750302.299017446\n",
      "-7953137.0 7933678.88364973\n"
     ]
    }
   ],
   "source": [
    "print(att3_1,se3_1)\n",
    "print(att3_2,se3_2)\n",
    "print(att3_3,se3_3)\n",
    "print(att3_4,se3_4)\n",
    "print(att3_5,se3_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
